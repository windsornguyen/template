{
  "model_type": "FlashSTU",
  "n_embd": 256,
  "n_heads": 8,
  "n_layers": 3,
  "seq_len": 8192,
  "window_size": 1024,
  "vocab_size": 200064,
  "mlp_scale": 12,
  "bias": false,
  "dropout": 0.0,
  "num_eigh": 24,
  "use_hankel_L": false,
  "num_epochs": 1,
  "global_bsz": 524288,
  "num_steps": 19073,
  "bsz": 4,
  "warmup_steps": 715,
  "eval_period": 128,
  "save_period": 128,
  "max_lr": 1.0e-3,
  "min_lr": 3.0e-5,
  "max_norm": 1.0,
  "dilation": 1,
  "fsdp": true,
  "ddp": false,
  "mixed_precision": true,
  "torch_dtype": "bfloat16",
  "use_cpu_offload": false,
  "sharding_strategy": "full_shard",
  "state_dict_type": "full",
  "auto_wrap_policy": "partial",
  "backward_prefetch": "backward_pre",
  "forward_prefetch": false,
  "sync_module_states": true,
  "use_orig_params": true,
  "device_id": null,
  "precision": {
    "param": "bfloat16",
    "reduce": "bfloat16",
    "buffer": "bfloat16"
  },
  "fsdp_modules": [
    "nn.Embedding",
    "STU",
    "Attention",
    "MLP",
    "nn.Linear"
  ],
  "use_activation_checkpointing": true,
  "use_flash_fft": true,
  "use_approx": false,
  "softcap": 50.0,
  "torch_compile": false
}
